<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>💐Preliminaries💐 | Dan's Blog</title><meta name="author" content="Dan"><meta name="copyright" content="Dan"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="[TOC] online readingPreliminariesData Manipulationdata manipulation vs. data preprocessingData Preprocessing: A comprehensive process that includes data manipulation to prepare raw data for analysis a">
<meta property="og:type" content="article">
<meta property="og:title" content="💐Preliminaries💐">
<meta property="og:url" content="https://danlovpotato.github.io/2024/06/21/MachineLearning/preliminaries/index.html">
<meta property="og:site_name" content="Dan&#39;s Blog">
<meta property="og:description" content="[TOC] online readingPreliminariesData Manipulationdata manipulation vs. data preprocessingData Preprocessing: A comprehensive process that includes data manipulation to prepare raw data for analysis a">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s2.loli.net/2023/02/10/rjhHVtISdz28Tk3.png">
<meta property="article:published_time" content="2024-06-21T04:00:00.000Z">
<meta property="article:modified_time" content="2024-06-22T19:36:22.010Z">
<meta property="article:author" content="Dan">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2023/02/10/rjhHVtISdz28Tk3.png"><link rel="shortcut icon" href="https://cdn.pixabay.com/photo/2016/03/31/19/26/cherry-blossom-1295009_1280.png"><link rel="canonical" href="https://danlovpotato.github.io/2024/06/21/MachineLearning/preliminaries/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '💐Preliminaries💐',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-06-22 15:36:22'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = url => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      link.onload = () => resolve()
      link.onerror = () => reject()
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2023/02/10/rjhHVtISdz28Tk3.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">55</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">10</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-paper-plane"></i><span> Messageboard</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Dan's Blog"><span class="site-name">Dan's Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-paper-plane"></i><span> Messageboard</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">💐Preliminaries💐</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-06-21T04:00:00.000Z" title="Created 2024-06-21 00:00:00">2024-06-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-06-22T19:36:22.010Z" title="Updated 2024-06-22 15:36:22">2024-06-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/ML/">ML</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="💐Preliminaries💐"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>[TOC]</p>
<h1 id="online-reading"><a href="#online-reading" class="headerlink" title="online reading"></a><a target="_blank" rel="noopener" href="https://d2l.ai/chapter_preliminaries/ndarray.html">online reading</a></h1><h1 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h1><h2 id="Data-Manipulation"><a href="#Data-Manipulation" class="headerlink" title="Data Manipulation"></a>Data Manipulation</h2><h3 id="data-manipulation-vs-data-preprocessing"><a href="#data-manipulation-vs-data-preprocessing" class="headerlink" title="data manipulation vs. data preprocessing"></a>data manipulation vs. data preprocessing</h3><p><strong>Data Preprocessing</strong>: A comprehensive process that includes data manipulation to prepare raw data for analysis and modeling.</p>
<p><strong>Data Manipulation</strong>: A subset of preprocessing tasks focused on transforming and organizing data.</p>
<p>data preprocessing includes data manipulation</p>
<h4 id="Data-Manipulation-1"><a href="#Data-Manipulation-1" class="headerlink" title="Data Manipulation"></a>Data Manipulation</h4><p><strong>Definition</strong>: Data manipulation refers to the process of changing data to make it more organized and easier to analyze. This includes various operations to transform the data.</p>
<p><strong>Common Tasks</strong>:</p>
<ol>
<li><strong>Merging and Joining</strong>: Combining data from multiple sources or tables.</li>
<li><strong>Sorting</strong>: Arranging data in a specific order.</li>
<li><strong>Filtering</strong>: Selecting a subset of data based on conditions.</li>
<li><strong>Aggregation</strong>: Summarizing data, such as calculating averages or sums.</li>
<li><strong>Reshaping</strong>: Changing the structure or format of data, such as pivoting tables.</li>
<li><strong>Indexing</strong>: Selecting specific rows or columns of data.</li>
<li><strong>Data Cleaning</strong>: Correcting or removing incorrect, corrupted, or duplicate data.</li>
</ol>
<h4 id="Data-Manipulation-2"><a href="#Data-Manipulation-2" class="headerlink" title="Data Manipulation"></a>Data Manipulation</h4><p><strong>Definition</strong>: Data manipulation refers to the process of changing data to make it more organized and easier to analyze. This includes various operations to transform the data.</p>
<p><strong>Common Tasks</strong>:</p>
<ol>
<li><strong>Merging and Joining</strong>: Combining data from multiple sources or tables.</li>
<li><strong>Sorting</strong>: Arranging data in a specific order.</li>
<li><strong>Filtering</strong>: Selecting a subset of data based on conditions.</li>
<li><strong>Aggregation</strong>: Summarizing data, such as calculating averages or sums.</li>
<li><strong>Reshaping</strong>: Changing the structure or format of data, such as pivoting tables.</li>
<li><strong>Indexing</strong>: Selecting specific rows or columns of data.</li>
<li><strong>Data Cleaning</strong>: Correcting or removing incorrect, corrupted, or duplicate data.</li>
</ol>
<h3 id="Saving-Memory"><a href="#Saving-Memory" class="headerlink" title="Saving Memory"></a>Saving Memory</h3><p>Python first evaluates Y + X, allocating new memory for the result and then points <code>Y</code> to this new location in memory.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example tensors</span></span><br><span class="line">Y = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">X = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save the original id of Y</span></span><br><span class="line">before = <span class="built_in">id</span>(Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Perform addition and reassignment</span></span><br><span class="line">Y = Y + X</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check if id(Y) remains the same after reassignment</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(Y) == before)  <span class="comment"># False, because Y now points to a new memory location</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output:</span></span><br><span class="line"><span class="comment"># False</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>False</p>
<p>We can assign the result of an operation to a previously allocated array <code>Y</code> by using slice notation: <code>Y[:] = &lt;expression&gt;</code>. To illustrate this concept, we overwrite the values of tensor <code>Z</code>, after initializing it, using <code>zeros_like</code>, to have the same shape as <code>Y</code>.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Z = torch.zeros_like(Y)</span><br><span class="line">print(&#x27;id(Z):&#x27;, id(Z))</span><br><span class="line">Z[:] = X + Y</span><br><span class="line">print(&#x27;id(Z):&#x27;, id(Z))</span><br></pre></td></tr></table></figure>

<p>id(Z): 140381179266448<br>id(Z): 140381179266448</p>
<p>If the value of <code>X</code> is not reused in subsequent computations, we can also use <code>X[:] = X + Y</code> or <code>X += Y</code> to reduce the memory overhead of the operation.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line"># Example tensors</span><br><span class="line">Y = torch.tensor([1, 2, 3])</span><br><span class="line">X = torch.tensor([4, 5, 6])</span><br><span class="line"></span><br><span class="line"># Save the original id of Y</span><br><span class="line">before = id(Y)</span><br><span class="line"></span><br><span class="line"># Perform in-place addition</span><br><span class="line">Y += X</span><br><span class="line"></span><br><span class="line"># Check if id(Y) remains the same after in-place addition</span><br><span class="line">print(id(Y) == before)  # True</span><br><span class="line"></span><br><span class="line"># Output:</span><br><span class="line"># True</span><br><span class="line">before = id(X)</span><br><span class="line">X += Y</span><br><span class="line">id(X) == before</span><br></pre></td></tr></table></figure>

<p>True</p>
<h3 id="Broadcasting"><a href="#Broadcasting" class="headerlink" title="Broadcasting"></a>Broadcasting</h3><p>Broadcasting is a feature in numerical libraries like NumPy and PyTorch that lets you perform operations on arrays of different shapes by automatically expanding smaller arrays to match the shape of larger ones. This process is efficient and avoids unnecessary memory usage.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># same shape </span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">a = np.array([1, 2, 3])</span><br><span class="line">b = np.array([4, 5, 6])</span><br><span class="line">print(a + b)  # Output: [5 7 9]</span><br><span class="line"></span><br><span class="line"># different shape</span><br><span class="line"></span><br><span class="line">a = np.array([1, 2, 3])</span><br><span class="line">b = np.array([4, 5, 6])</span><br><span class="line">print(a + b)  # Output: [5 7 9]</span><br><span class="line"></span><br><span class="line">a = np.array([1, 2, 3])</span><br><span class="line">b = 2</span><br><span class="line">print(a + b)  # Output: [3 4 5]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = np.array([[1], [2], [3]])</span><br><span class="line">b = np.array([4, 5, 6])</span><br><span class="line">print(a + b)</span><br><span class="line"># Output:</span><br><span class="line"># [[5 6 7]      +1</span><br><span class="line">#  [6 7 8]       +2</span><br><span class="line">#  [7 8 9]]       +3</span><br></pre></td></tr></table></figure>

<h3 id="Conversion-to-Other-Python-Objects"><a href="#Conversion-to-Other-Python-Objects" class="headerlink" title="Conversion to Other Python Objects"></a>Conversion to Other Python Objects</h3><p>Converting to a NumPy tensor (<code>ndarray</code>), or vice versa, is easy. The torch tensor and NumPy array will share their underlying memory, and changing one through an in-place operation will also change the other.</p>
<p>Numpy Array   — —- —- PyTorch Tensor</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">A = X.numpy()</span><br><span class="line">B = torch.tensor(A)</span><br><span class="line">type(A), type(B)</span><br><span class="line"></span><br><span class="line"># output: (numpy.ndarray, torch.Tensor)</span><br></pre></td></tr></table></figure>

<p>To convert a size-1 tensor to a Python scalar, we can invoke the <code>item</code> function or Python’s built-in functions.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([3.5])</span><br><span class="line">a, a.item(), float(a), int(a)</span><br><span class="line"># (tensor([3.5000]), 3.5, 3.5, 3)</span><br></pre></td></tr></table></figure>



<h2 id="Data-Processing"><a href="#Data-Processing" class="headerlink" title="Data Processing"></a>Data Processing</h2><p>Comma-separated values (CSV) files are ubiquitous for the storing of tabular (spreadsheet-like) data.</p>
<p>CSV files and Excel sheets both store tabular data, but they have key differences:</p>
<h3 id="CSV-Files"><a href="#CSV-Files" class="headerlink" title="CSV Files"></a>CSV Files</h3><ul>
<li><strong>Format</strong>: Plain text format where data is separated by commas.</li>
<li><strong>File Extension</strong>: <code>.csv</code></li>
<li><strong>Content</strong>: Only contains data, no formatting, formulas, or multimedia.</li>
<li><strong>Compatibility</strong>: Can be opened by any text editor, spreadsheet software, or program that supports CSV.</li>
<li><strong>Size</strong>: Typically smaller because it lacks additional features.</li>
</ul>
<h3 id="Excel-Sheets"><a href="#Excel-Sheets" class="headerlink" title="Excel Sheets"></a>Excel Sheets</h3><ul>
<li><strong>Format</strong>: Binary or XML-based format for Microsoft Excel.</li>
<li><strong>File Extensions</strong>: <code>.xls</code> (older format), <code>.xlsx</code> (newer format)</li>
<li><strong>Content</strong>: Contains data, but also supports complex formatting, formulas, charts, and multimedia.</li>
<li><strong>Compatibility</strong>: Best opened with Excel or similar spreadsheet software (like Google Sheets or LibreOffice Calc).</li>
<li><strong>Features</strong>: Supports advanced features like pivot tables, macros, and data validation.</li>
</ul>
<p> Summary</p>
<p>CSV files are simple and lightweight for storing plain data, while Excel sheets offer rich features for data manipulation and presentation.</p>
<h2 id="Linear-Algebra"><a href="#Linear-Algebra" class="headerlink" title="Linear Algebra"></a>Linear Algebra</h2><h3 id="Scalars"><a href="#Scalars" class="headerlink" title="Scalars"></a>Scalars</h3><p>严格来说，仅包含一个数值被称为<em>标量</em>（scalar）。</p>
<p>We denote scalars by ordinary lower-cased letters .</p>
<p>Scalars are implemented as tensors that contain only one element.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">3.0</span>)</span><br><span class="line">y = torch.tensor(<span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line">x + y, x * y, x / y, x**y</span><br><span class="line"><span class="comment">#(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))</span></span><br></pre></td></tr></table></figure>

<h3 id="Vectors"><a href="#Vectors" class="headerlink" title="Vectors"></a>Vectors</h3><p>you can think of a vector as a fixed-length array of scalars. </p>
<p>For example, if we were training a model to predict the risk of a loan defaulting, we might associate each applicant with a vector whose components correspond to quantities like their income, length of employment, or number of previous defaults.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(3)</span><br><span class="line">x</span><br><span class="line"></span><br><span class="line">#tensor([0, 1, 2])</span><br></pre></td></tr></table></figure>

<h3 id="Matrices"><a href="#Matrices" class="headerlink" title="Matrices"></a>Matrices</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">A = torch.arange(6).reshape(3, 2)</span><br><span class="line">A</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor([[0, 1],</span><br><span class="line">        [2, 3],</span><br><span class="line">        [4, 5]])</span><br><span class="line">        </span><br><span class="line">A.T</span><br><span class="line"></span><br><span class="line">tensor([[0, 2, 4],</span><br><span class="line">        [1, 3, 5]])</span><br><span class="line">        </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><p>While you can go far in your machine learning journey with only scalars, vectors, and matrices, eventually you may need to work with higher-order tensors. </p>
<p>Tensors give us a generic way of describing extensions to n th-order arrays.</p>
<h3 id="Basic-Properties-of-Tensor-Arithmetic"><a href="#Basic-Properties-of-Tensor-Arithmetic" class="headerlink" title="Basic Properties of Tensor Arithmetic"></a>Basic Properties of Tensor Arithmetic</h3><p>The elementwise product of two matrices is called their <em>Hadamard product</em> (denoted ⊙)</p>
<p>Matrix Product vs. Hadamard Product</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="D:\HuaweiMoveData\Users\XIANGDANTONG\Documents\blog\source_posts\MachineLearning\img\product.png" alt="product"></p>
<h3 id="Reduction-sum"><a href="#Reduction-sum" class="headerlink" title="Reduction (sum)"></a>Reduction (sum)</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">A: (array([[ 0.,  1.,  2.,  3.],</span><br><span class="line">        [ 4.,  5.,  6.,  7.],</span><br><span class="line">        [ 8.,  9., 10., 11.],</span><br><span class="line">        [12., 13., 14., 15.],</span><br><span class="line">        [16., 17., 18., 19.]]),</span><br></pre></td></tr></table></figure>

<p>0 columns </p>
<p>1 rows</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A_sum_axis1 = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>) </span><br><span class="line">A_sum_axis1, A_sum_axis1.shape</span><br><span class="line"><span class="comment">#(array([ 6., 22., 38., 54., 70.]), (5,))</span></span><br><span class="line"></span><br><span class="line">A_sum_axis0 = A.<span class="built_in">sum</span>(axis=<span class="number">0</span>)</span><br><span class="line">A_sum_axis0, A_sum_axis0.shape</span><br><span class="line"><span class="comment"># (array([40., 45., 50., 55.]), (4,))</span></span><br></pre></td></tr></table></figure>

<h3 id="Non-Reduction-Sum"><a href="#Non-Reduction-Sum" class="headerlink" title="Non-Reduction Sum"></a>Non-Reduction Sum</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">sum_A = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">sum_A</span><br><span class="line"></span><br><span class="line">array([[ <span class="number">6.</span>],</span><br><span class="line">       [<span class="number">22.</span>],</span><br><span class="line">       [<span class="number">38.</span>],</span><br><span class="line">       [<span class="number">54.</span>],</span><br><span class="line">       [<span class="number">70.</span>]]) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">A / sum_A</span><br><span class="line"></span><br><span class="line">array([[<span class="number">0.</span>        , <span class="number">0.16666667</span>, <span class="number">0.33333334</span>, <span class="number">0.5</span>       ],</span><br><span class="line">       [<span class="number">0.18181819</span>, <span class="number">0.22727273</span>, <span class="number">0.27272728</span>, <span class="number">0.3181818</span> ],</span><br><span class="line">       [<span class="number">0.21052632</span>, <span class="number">0.23684211</span>, <span class="number">0.2631579</span> , <span class="number">0.28947368</span>],</span><br><span class="line">       [<span class="number">0.22222222</span>, <span class="number">0.24074075</span>, <span class="number">0.25925925</span>, <span class="number">0.2777778</span> ],</span><br><span class="line">       [<span class="number">0.22857143</span>, <span class="number">0.24285714</span>, <span class="number">0.25714287</span>, <span class="number">0.27142859</span>]])</span><br></pre></td></tr></table></figure>

<p>When <code>keepdims=True</code> is used with NumPy’s <code>sum()</code> function along a specified axis, it affects the shape of the resulting array by retaining the dimensions of the summed axis. Here’s how it works:</p>
<p>累加</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A.cumsum(axis=0)</span><br><span class="line"></span><br><span class="line">array([[ 0.,  1.,  2.,  3.],</span><br><span class="line">       [ 4.,  6.,  8., 10.],</span><br><span class="line">       [12., 15., 18., 21.],</span><br><span class="line">       [24., 28., 32., 36.],</span><br><span class="line">       [40., 45., 50., 55.]])</span><br></pre></td></tr></table></figure>

<h3 id="dot-product"><a href="#dot-product" class="headerlink" title="dot product"></a>dot product</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define matrices A and B for matrix product</span></span><br><span class="line">A = np.array([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">              [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line">B = np.array([[<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">              [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Matrix product using np.dot or @ operator</span></span><br><span class="line">AB = np.dot(A, B)</span><br><span class="line"><span class="comment"># Or equivalently: AB = A @ B</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Matrix Product AB:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(AB)</span><br><span class="line"></span><br><span class="line">Matrix Product AB:</span><br><span class="line">[[<span class="number">19</span> <span class="number">22</span>]</span><br><span class="line"> [<span class="number">43</span> <span class="number">50</span>]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-----------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define matrices A and B for Hadamard product (element-wise multiplication)</span></span><br><span class="line">A = np.array([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">              [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line">B = np.array([[<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">              [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Element-wise multiplication (Hadamard product)</span></span><br><span class="line">A_hadamard_B = A * B</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hadamard Product A ⊙ B:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(A_hadamard_B)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Hadamard Product A ⊙ B:</span><br><span class="line">[[ <span class="number">5</span> <span class="number">12</span>]</span><br><span class="line"> [<span class="number">21</span> <span class="number">32</span>]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Norms"><a href="#Norms" class="headerlink" title="Norms"></a>Norms</h3><p>the norm of a vector tells us how <em>big</em> it is</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example vector x</span></span><br><span class="line">x = np.array([<span class="number">3</span>, -<span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">x_torch = torch.tensor([<span class="number">3.</span>, -<span class="number">4.</span>, <span class="number">5.</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute L1 norm</span></span><br><span class="line">norm_l1_np = np.linalg.norm(x, <span class="built_in">ord</span>=<span class="number">1</span>)</span><br><span class="line">norm_l1_torch = torch.norm(x_torch, p=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute L2 norm</span></span><br><span class="line">norm_l2_np = np.linalg.norm(x, <span class="built_in">ord</span>=<span class="number">2</span>)</span><br><span class="line">norm_l2_torch = torch.norm(x_torch, p=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Vector x:&quot;</span>, x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;L1 Norm (NumPy):&quot;</span>, norm_l1_np)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;L1 Norm (PyTorch):&quot;</span>, norm_l1_torch.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;L2 Norm (NumPy):&quot;</span>, norm_l2_np)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;L2 Norm (PyTorch):&quot;</span>, norm_l2_torch.item())</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Output:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Vector x: [ 3 -4  5]</span><br><span class="line">L1 Norm (NumPy): 12.0</span><br><span class="line">L1 Norm (PyTorch): 12.0</span><br><span class="line">L2 Norm (NumPy): 7.0710678118654755</span><br><span class="line">L2 Norm (PyTorch): 7.071067810058594</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>L1 Norm (Manhattan Norm)</strong>: Computes the sum of the absolute values of the vector elements. It measures the distance a taxi would travel in a city grid system.</p>
<p><strong>L2 Norm (Euclidean Norm)</strong>: Computes the square root of the sum of the squares of the vector elements. It measures the straight-line distance between two points in Euclidean space.</p>
<h2 id="Automatic-Differentiation"><a href="#Automatic-Differentiation" class="headerlink" title="Automatic Differentiation"></a>Automatic Differentiation</h2><p>自动微分</p>
<p><strong>Create Tensor <code>x</code></strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(4.0)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<p>Output:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">tensor([0., 1., 2., 3.])</span><br></pre></td></tr></table></figure>

<p><code>x</code> is a tensor with values <code>[0., 1., 2., 3.]</code>.</p>
<p><strong>Dot Product of <code>x</code> with Itself</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">torch.dot(x, x)</span><br></pre></td></tr></table></figure>

<p>The dot product of a vector with itself is calculated as follows:</p>
<p>torch.dot(x,x)&#x3D;x[0]⋅x[0]+x[1]⋅x[1]+x[2]⋅x[2]+x[3]⋅x[3]</p>
<p>Plugging in the values from <code>x</code>:</p>
<p>torch.dot(x,x)&#x3D;0⋅0+1⋅1+2⋅2+3⋅3&#x3D;0+1+4+9&#x3D;14‘ &#x3D; 0 + 1 + 4 + 9 &#x3D; 14 </p>
<p><strong>Multiply by 2</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y = 2 * torch.dot(x, x)</span><br><span class="line">print(y)</span><br><span class="line"># y = 2 * (x[0]^2 + x[1]^2 + x[2]^2 + x[3]^2)</span><br></pre></td></tr></table></figure>

<p>We then multiply the result of the dot product by 2:</p>
<p>y&#x3D;2⋅14&#x3D;28</p>
<p><strong>Result</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y)</span><br></pre></td></tr></table></figure>

<p>Output:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(28., grad_fn=&lt;MulBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>We can now take the gradient of <code>y</code> with respect to <code>x</code> by calling its <code>backward</code> method. Next, we can access the gradient via <code>x</code>’s <code>grad</code> attribute.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 0.,  4.,  8., 12.])</span><br></pre></td></tr></table></figure>

<p>Now let’s calculate another function of <code>x</code> and take its gradient. Note that PyTorch does not automatically reset the gradient buffer when we record a new gradient. Instead, the new gradient is added to the already-stored gradient. This behavior comes in handy when we want to optimize the sum of multiple objective functions. To reset the gradient buffer, we can call <code>x.grad.zero_()</code> as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()  # Reset the gradient</span><br><span class="line">y = x.sum()</span><br><span class="line">y.backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1., 1., 1., 1.])</span><br></pre></td></tr></table></figure>

<p><strong><code>y.backward()</code></strong>: This function call computes gradients using the chain rule of calculus. It calculates ∂y&#x2F;∂xi for each element xi in <code>x</code> and stores these gradients in <code>x.grad</code>.</p>
<p><strong><code>x.grad</code></strong>: After calling <code>y.backward()</code>, <code>x.grad</code> will be <code>[1.0, 1.0, 1.0]</code>, because ∂y&#x2F;∂xi &#x3D; 1 for each element of <code>x</code> when <code>y = sum(x)</code>.</p>
<hr>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x</span><br><span class="line">y.backward(gradient=torch.ones(len(y)))  # Faster: y.sum().backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0., 2., 4., 6.])</span><br></pre></td></tr></table></figure>

<h3 id="Detaching-Computation"><a href="#Detaching-Computation" class="headerlink" title="Detaching Computation"></a>Detaching Computation</h3><p><a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/tensor-detach-method-in-python-pytorch/">https://www.geeksforgeeks.org/tensor-detach-method-in-python-pytorch/</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x</span><br><span class="line">u = y.detach()</span><br><span class="line">z = u * x</span><br><span class="line"></span><br><span class="line">z.sum().backward()</span><br><span class="line">x.grad == u</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([True, True, True, True])</span><br></pre></td></tr></table></figure>

<p>因此，尽管 <code>u = y.detach()</code> 分离了 <code>y</code>，使得 <code>u</code> 不再具有梯度信息，但 <code>u</code> 仍然是一个张量。它与 <code>y</code> 共享相同的数据，但不再与计算图相关联，因此在反向传播过程中不会传播梯度到 <code>u</code>。</p>
<p><code>u</code> 是 <code>y</code> 的一个副本，但不再与计算图相关联，因此不会进行梯度跟踪。</p>
<p>假设 <code>x</code> 的初始值为 <code>[1.0, 2.0, 3.0]</code>，那么：</p>
<ul>
<li><code>y = [1.0^2, 2.0^2, 3.0^2] = [1.0, 4.0, 9.0]</code></li>
<li><code>u = [1.0, 4.0, 9.0]</code> （因为 <code>u</code> 是 <code>y</code> 的副本）</li>
<li><code>z = u * x = [1.0 * 1.0, 4.0 * 2.0, 9.0 * 3.0] = [1.0, 8.0, 27.0]</code></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y.sum().backward()</span><br><span class="line">x.grad == 2 * x</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([True, True, True, True])</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x</span><br><span class="line">u = y.detach()</span><br><span class="line">z = u * x</span><br><span class="line"></span><br><span class="line">z.sum().backward()</span><br></pre></td></tr></table></figure>

<ul>
<li><code>u</code> 是通过 <code>detach()</code> 方法从 <code>y</code> 分离出来的一个张量。这意味着 <code>u</code> 是一个与 <code>y</code> 具有相同数值的张量，但是它不再与计算图关联，也不再跟踪梯度。因此，可以将 <code>u</code> 视为一个常数张量。</li>
<li>当执行 <code>z.sum().backward()</code> 时，PyTorch 计算 <code>z</code> 的和 <code>z.sum()</code> 对 <code>x</code> 的梯度。因为 <code>z = u * x</code>，所以 <code>z</code> 的梯度 &#x3D; u<ul>
<li><code>z.sum()</code> 对 <code>x[i]</code> 的梯度是 <code>u[i]</code>，因为在计算图中，<code>u</code> 被视为一个常数，而不是变量。</li>
</ul>
</li>
</ul>
<p>因此，<code>x.grad</code> 包含的值 <code>[2.0, 4.0, 6.0]</code> 是根据这个推理得出的.</p>
<h3 id="Dynamic-control-flow"><a href="#Dynamic-control-flow" class="headerlink" title="Dynamic control flow"></a>Dynamic control flow</h3><p>控制流的梯度计算</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def f(a):</span><br><span class="line">    b = a * 2</span><br><span class="line">    while b.norm() &lt; 1000:</span><br><span class="line">        b = b * 2</span><br><span class="line">    if b.sum() &gt; 0:</span><br><span class="line">        c = b</span><br><span class="line">    else:</span><br><span class="line">        c = 100 * b</span><br><span class="line">    return c</span><br></pre></td></tr></table></figure>



<p>让我们计算梯度。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(size=(), requires_grad=True)</span><br><span class="line">d = f(a)</span><br><span class="line">d.backward()</span><br></pre></td></tr></table></figure>



<p>我们现在可以分析上面定义的<code>f</code>函数。 请注意，它在其输入<code>a</code>中是分段线性的。 换言之，对于任何<code>a</code>，存在某个常量标量<code>k</code>，使得<code>f(a)=k*a</code>，其中<code>k</code>的值取决于输入<code>a</code>，因此可以用<code>d/a</code>验证梯度是否正确。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.grad == d / a</span><br></pre></td></tr></table></figure>

<p>Dynamic control flow in the context of deep learning frameworks like PyTorch means that the computation and flow of operations can depend on the input data or conditions encountered during runtime. Unlike static control flow where the computational graph is predetermined and fixed before execution, dynamic control flow allows for flexibility in how operations are executed based on the actual data being processed.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://DanLovPotato.github.io">Dan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://danlovpotato.github.io/2024/06/21/MachineLearning/preliminaries/">https://danlovpotato.github.io/2024/06/21/MachineLearning/preliminaries/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://s2.loli.net/2023/02/10/rjhHVtISdz28Tk3.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/06/22/MachineLearning/LinearNeuralNetworksForRegression/" title="💐Linear Neural Networks for Regression💐"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">💐Linear Neural Networks for Regression💐</div></div></a></div><div class="next-post pull-right"><a href="/2024/06/15/Leetcode-2Month/Backtracking/" title="Backtracking"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Backtracking</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2023/02/10/rjhHVtISdz28Tk3.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Dan</div><div class="author-info__description">Lack the words to compliment myself today.</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">55</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">10</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/DanLovPotato"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#online-reading"><span class="toc-number">1.</span> <span class="toc-text">online reading</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Preliminaries"><span class="toc-number">2.</span> <span class="toc-text">Preliminaries</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Data-Manipulation"><span class="toc-number">2.1.</span> <span class="toc-text">Data Manipulation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#data-manipulation-vs-data-preprocessing"><span class="toc-number">2.1.1.</span> <span class="toc-text">data manipulation vs. data preprocessing</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Data-Manipulation-1"><span class="toc-number">2.1.1.1.</span> <span class="toc-text">Data Manipulation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Data-Manipulation-2"><span class="toc-number">2.1.1.2.</span> <span class="toc-text">Data Manipulation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Saving-Memory"><span class="toc-number">2.1.2.</span> <span class="toc-text">Saving Memory</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Broadcasting"><span class="toc-number">2.1.3.</span> <span class="toc-text">Broadcasting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conversion-to-Other-Python-Objects"><span class="toc-number">2.1.4.</span> <span class="toc-text">Conversion to Other Python Objects</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Data-Processing"><span class="toc-number">2.2.</span> <span class="toc-text">Data Processing</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CSV-Files"><span class="toc-number">2.2.1.</span> <span class="toc-text">CSV Files</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Excel-Sheets"><span class="toc-number">2.2.2.</span> <span class="toc-text">Excel Sheets</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Linear-Algebra"><span class="toc-number">2.3.</span> <span class="toc-text">Linear Algebra</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Scalars"><span class="toc-number">2.3.1.</span> <span class="toc-text">Scalars</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Vectors"><span class="toc-number">2.3.2.</span> <span class="toc-text">Vectors</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Matrices"><span class="toc-number">2.3.3.</span> <span class="toc-text">Matrices</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tensor"><span class="toc-number">2.3.4.</span> <span class="toc-text">Tensor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Basic-Properties-of-Tensor-Arithmetic"><span class="toc-number">2.3.5.</span> <span class="toc-text">Basic Properties of Tensor Arithmetic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reduction-sum"><span class="toc-number">2.3.6.</span> <span class="toc-text">Reduction (sum)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Non-Reduction-Sum"><span class="toc-number">2.3.7.</span> <span class="toc-text">Non-Reduction Sum</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dot-product"><span class="toc-number">2.3.8.</span> <span class="toc-text">dot product</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Norms"><span class="toc-number">2.3.9.</span> <span class="toc-text">Norms</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Automatic-Differentiation"><span class="toc-number">2.4.</span> <span class="toc-text">Automatic Differentiation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Detaching-Computation"><span class="toc-number">2.4.1.</span> <span class="toc-text">Detaching Computation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dynamic-control-flow"><span class="toc-number">2.4.2.</span> <span class="toc-text">Dynamic control flow</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/24/Leetcode-2Month/Greedy/" title="Greedy">Greedy</a><time datetime="2024-06-24T04:00:00.000Z" title="Created 2024-06-24 00:00:00">2024-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/22/MachineLearning/LinearNeuralNetworksForRegression/" title="💐Linear Neural Networks for Regression💐">💐Linear Neural Networks for Regression💐</a><time datetime="2024-06-22T04:00:00.000Z" title="Created 2024-06-22 00:00:00">2024-06-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/21/MachineLearning/preliminaries/" title="💐Preliminaries💐">💐Preliminaries💐</a><time datetime="2024-06-21T04:00:00.000Z" title="Created 2024-06-21 00:00:00">2024-06-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/15/Leetcode-2Month/Backtracking/" title="Backtracking">Backtracking</a><time datetime="2024-06-15T04:00:00.000Z" title="Created 2024-06-15 00:00:00">2024-06-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/07/GaussianSplatting/code/" title="DOF-GS with Joint Camera Optimization">DOF-GS with Joint Camera Optimization</a><time datetime="2024-06-07T04:00:00.000Z" title="Created 2024-06-07 00:00:00">2024-06-07</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Dan</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>